{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f416e9",
      "metadata": {
        "id": "c5f416e9",
        "outputId": "cbe7ea0c-0b1a-4149-f9c5-ac19e05e3567"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/02 14:00:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "# very important to keep number of partitions low, initially 3300 partitions of\n",
        "# 70 Mbs each, reduced to 417 with below config, working with 3300 partitions\n",
        "# resulted in 5x more compute time due to a lot of network I/O and\n",
        "# led to less workers being shut down\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100) # this reads data in 1 Mb chunks\n",
        "    # because there is a rate limit on reading data from GCP buckets on trial accounts.\n",
        "    .getOrCreate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368fb0fa",
      "metadata": {
        "id": "368fb0fa",
        "outputId": "e9791583-d54b-453b-8106-fd311ca6b32e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df = spark.read.parquet(\"gs://bigdata_27/features/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6716f3c1",
      "metadata": {
        "id": "6716f3c1",
        "outputId": "c046dba6-3b37-466e-c745-4186410b64ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- canonical_smiles: string (nullable = true)\n",
            " |-- drug: string (nullable = true)\n",
            " |-- moa-fine: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247bcd15",
      "metadata": {
        "id": "247bcd15",
        "outputId": "544dc14e-6f88-4160-abbe-4197efb716b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4096m'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/30 22:41:17 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:20 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:23 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:26 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:29 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:32 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:35 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:38 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:41 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:44 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:47 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:50 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:53 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:56 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:41:59 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:02 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:05 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:08 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:11 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:14 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:17 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:20 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:23 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:26 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:29 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:32 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:35 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:38 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:41 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:44 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:47 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:50 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:53 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:56 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:42:59 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:02 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:05 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:08 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:11 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:14 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:17 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:20 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:23 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:26 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:29 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:32 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:35 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:38 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:41 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:44 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:47 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:50 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:53 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:56 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:43:59 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:44:02 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:44:05 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n",
            "25/04/30 22:44:07 WARN YarnAllocatorNodeHealthTracker: No available nodes reported, please check Resource Manager.\n"
          ]
        }
      ],
      "source": [
        "spark.sparkContext.getConf().get(\"spark.driver.memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd4981c",
      "metadata": {
        "id": "bdd4981c",
        "outputId": "47c65e9f-d167-4c2c-9412-c2251f778452"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VarianceThresholdSelector\n",
        "\n",
        "selector = VarianceThresholdSelector(\n",
        "    featuresCol=\"features\",         # your SparseVector column\n",
        "    outputCol=\"selected_features\",\n",
        "    varianceThreshold=0.1       # keep only features with variance > 0.01\n",
        ")\n",
        "selected_df = selector.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5bc3cf4",
      "metadata": {
        "id": "d5bc3cf4",
        "outputId": "cde3751e-9dd0-4e67-e992-7646f5f38ace"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "selected_df.drop(\"features\").write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://bigdata_27/scaled_features/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d8654a",
      "metadata": {
        "id": "78d8654a",
        "outputId": "fe7b30df-66c0-4465-cc4e-9c00717b56cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+--------+--------------------+\n",
            "|    canonical_smiles|       drug|moa-fine|   selected_features|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "|CC(C)NC(=O)COC1=C...|Belumosudil| unclear|(6256,[26,35,36,4...|\n",
            "+--------------------+-----------+--------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "scaled_df = spark.read.parquet(\"gs://bigdata_27/scaled_features/\")\n",
        "scaled_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d67f629",
      "metadata": {
        "id": "1d67f629",
        "outputId": "7b92e02c-81ed-4455-9c8c-62c6d4505bc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "95624334"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7c3d7f",
      "metadata": {
        "id": "9f7c3d7f"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "#df_100 = spark.read.parquet(\"gs://bigdata_27/scaled_features/part-02493-aae67da7-d1d8-4b34-9535-493843b5f419-c000.snappy.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cd68ce",
      "metadata": {
        "id": "a1cd68ce",
        "outputId": "1bcc60b5-f261-4b1a-940a-c91c57c8d5c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "29956"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_100.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989d0fac",
      "metadata": {
        "id": "989d0fac",
        "outputId": "eff09ea2-8e03-4003-cc6e-dc230eddd0a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply PCA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# adjust k as needed\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save the PCA model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pca_model\u001b[38;5;241m.\u001b[39mwrite()\u001b[38;5;241m.\u001b[39moverwrite()\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://bigdata_27/pca_models/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Apply PCA\n",
        "pca = PCA(k=256, inputCol=\"selected_features\", outputCol=\"pca_features\")  # adjust k as needed\n",
        "pca_model = pca.fit(scaled_df)\n",
        "\n",
        "# Save the PCA model\n",
        "pca_model.write().overwrite().save(\"gs://bigdata_27/pca_models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcbf7f1",
      "metadata": {
        "id": "2dcbf7f1",
        "outputId": "54efdae8-153f-42bd-bda3-7fb4b0a6a863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "fractions = scaled_df.select(\"moa-fine\").distinct().withColumn(\"fraction\", lit(0.01)).rdd.collectAsMap()\n",
        "sampled_df = scaled_df.sampleBy(\"moa-fine\", fractions, seed=42).coalesce(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28a5795",
      "metadata": {
        "id": "f28a5795",
        "outputId": "8315a59b-a266-4cbe-c2b6-65ee7bc37b86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "954884"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f544cc",
      "metadata": {
        "id": "38f544cc",
        "outputId": "5dceb98f-4282-424e-9665-bcd510390bc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "sampled_df\\\n",
        "  .write \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .parquet(\"gs://bigdata_27/one_percent_subset/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8668b2b",
      "metadata": {
        "id": "c8668b2b",
        "outputId": "88c7a8e4-8d7b-4a72-dc20-a0844347e39a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 9:>                                                          (0 + 1) / 1]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "sampled_df = spark.read.parquet(\"gs://bigdata_27/one_percent_subset/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fa2c75",
      "metadata": {
        "id": "74fa2c75",
        "outputId": "52a69dc7-8649-40de-e73e-96e2692404bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[canonical_smiles: string, drug: string, moa-fine: string, selected_features: vector]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d0004e",
      "metadata": {
        "id": "61d0004e",
        "outputId": "c1234c41-db99-4691-dc08-6058321a3dd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 12:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|    canonical_smiles|                drug|            moa-fine|   selected_features|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|CC1C(C(CC(O1)OC2C...|Epirubicin (hydro...|DNA synthesis/rep...|(6256,[0,34,69,78...|\n",
            "|CC1C(C(CC(O1)OC2C...|Epirubicin (hydro...|DNA synthesis/rep...|(6256,[2,9,12,21,...|\n",
            "|CC1C(C(CC(O1)OC2C...|Epirubicin (hydro...|DNA synthesis/rep...|(6256,[0,3,4,8,10...|\n",
            "|CC1C(C(CC(O1)OC2C...|Epirubicin (hydro...|DNA synthesis/rep...|(6256,[1,4,8,19,2...|\n",
            "|CC1C(C(CC(O1)OC2C...|Epirubicin (hydro...|DNA synthesis/rep...|(6256,[6,9,25,42,...|\n",
            "|C1=CC=C(C=C1)C2(C...|  Phenytoin (sodium)|             unclear|(6256,[5,7,8,24,2...|\n",
            "|C1=CC=C(C=C1)C2(C...|  Phenytoin (sodium)|             unclear|(6256,[9,32,45,50...|\n",
            "|C1=CC=C(C=C1)C2(C...|  Phenytoin (sodium)|             unclear|(6256,[5,12,25,34...|\n",
            "|CCCCCCCCCCCC(CC1C...|            Orlistat|             unclear|(6256,[3,14,24,25...|\n",
            "|CCCCCCCCCCCC(CC1C...|            Orlistat|             unclear|(6256,[3,7,13,20,...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "sampled_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5163d014",
      "metadata": {
        "id": "5163d014"
      },
      "outputs": [],
      "source": [
        "selected_df.drop(\"features\").write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://bigdata_27/scaled_features/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27163b0b",
      "metadata": {
        "id": "27163b0b",
        "outputId": "91dbc5b6-94b8-4e8d-9637-45c600e46ce1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 15:52:17 WARN TaskSetManager: Stage 19 contains a task of very large size (12780 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Apply PCA\n",
        "pca = PCA(k=256, inputCol=\"selected_features\", outputCol=\"pca_features\")  # adjust k as needed\n",
        "pca_model = pca.fit(sampled_df)\n",
        "\n",
        "# Save the PCA model\n",
        "pca_model.write().overwrite().save(\"gs://bigdata_27/pca_models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef69ccc",
      "metadata": {
        "id": "aef69ccc"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2e9837",
      "metadata": {
        "id": "0c2e9837",
        "outputId": "799efb78-d083-471c-8aaa-8660a8aae775"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import PCAModel\n",
        "\n",
        "\n",
        "pca_model = PCAModel.load(\"gs://bigdata_27/pca_models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb511a8",
      "metadata": {
        "id": "eeb511a8"
      },
      "outputs": [],
      "source": [
        "sampled_df = pca_model.transform(sampled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45238ad",
      "metadata": {
        "id": "e45238ad",
        "outputId": "1f870e6a-a2f9-4540-97c2-23693a7d493a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 16:04:16 WARN DAGScheduler: Broadcasting large task binary with size 12.3 MiB\n",
            "[Stage 28:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|        pca_features|\n",
            "+--------------------+\n",
            "|[-37.014480892292...|\n",
            "+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "sampled_df.select(\"pca_features\").show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a8c5c78",
      "metadata": {
        "id": "1a8c5c78",
        "outputId": "cdfb8c15-314e-4c06-e43a-9504df03a738"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 16:06:59 WARN DAGScheduler: Broadcasting large task binary with size 12.3 MiB\n",
            "[Stage 30:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PCA vector length: <class 'pyspark.ml.linalg.DenseVector'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "first_row = sampled_df.select(\"pca_features\").head()\n",
        "vector_length = type(first_row[\"pca_features\"])\n",
        "print(f\"PCA vector length: {vector_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a160c455",
      "metadata": {
        "id": "a160c455",
        "outputId": "2aac9ddb-c715-4482-e8a8-1cf55db5f34c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 16:22:39 WARN DAGScheduler: Broadcasting large task binary with size 12.5 MiB\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "pca_model.transform(scaled_df).drop(\"selected_features\").write.mode(\"overwrite\").parquet(\"gs://bigdata_27/transformed_data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784fd8a5",
      "metadata": {
        "id": "784fd8a5",
        "outputId": "3065bbaf-a134-4d05-d648-5cc24942b55f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|    canonical_smiles|                drug|            moa-fine|        pca_features|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|CNCC(C1=CC(=CC=C1...|Phenylephrine (hy...|Adrenoceptor agonist|[-30.062736918028...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "transformed_df = spark.read.parquet(\"gs://bigdata_27/transformed_data/\")\n",
        "transformed_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17febaba",
      "metadata": {
        "id": "17febaba",
        "outputId": "0421703e-9c52-4d73-f83a-aa0276551652"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 19:22:53 WARN TaskSetManager: Lost task 0.0 in stage 38.0 (TID 3754) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00015-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\n",
            "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/05/01 19:22:53 WARN TaskSetManager: Lost task 1.0 in stage 38.0 (TID 3755) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\n",
            "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/05/01 19:22:53 WARN TaskSetManager: Lost task 2.0 in stage 38.0 (TID 3756) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00026-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\n",
            "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 38:>                                                       (0 + 2) / 370]\r",
            "25/05/01 19:22:53 ERROR TaskSetManager: Task 1 in stage 38.0 failed 4 times; aborting job\n",
            "25/05/01 19:22:53 WARN TaskSetManager: Lost task 0.2 in stage 38.0 (TID 3761) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 38.0 failed 4 times, most recent failure: Lost task 1.3 in stage 38.0 (TID 3762) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\n",
            "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/05/01 19:22:53 WARN TaskSetManager: Lost task 2.2 in stage 38.0 (TID 3763) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 38.0 failed 4 times, most recent failure: Lost task 1.3 in stage 38.0 (TID 3762) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\n",
            "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
            "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o248.javaToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 38.0 failed 4 times, most recent failure: Lost task 1.3 in stage 38.0 (TID 3762) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fractions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoa-fine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39mcollectAsMap()\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:214\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m<class 'pyspark.rdd.RDD'>\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[1;32m    216\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o248.javaToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 38.0 failed 4 times, most recent failure: Lost task 1.3 in stage 38.0 (TID 3762) (cluster-f229-w-0.us-central1-a.c.big-data-attempt-2-457220.internal executor 63): org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File not found: gs://bigdata_27/transformed_data/part-00020-ce27b2b4-6888-415f-957e-3836700542d2-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:656)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "source": [
        "fractions = transformed_df.select(\"moa-fine\").distinct().withColumn(\"fraction\", lit(0.95)).rdd.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6c8f6b",
      "metadata": {
        "id": "3c6c8f6b",
        "outputId": "d2d8851a-bc70-4033-bbb4-9bdadfb84ca7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "transformed_df.sampleBy(\"moa-fine\", fractions, seed=42).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://bigdata_27/transformed_data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e109521e",
      "metadata": {
        "id": "e109521e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "string_cols = [\"canonical_smiles\",\"drug\",\"moa-fine\"]\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col + \"_indexed\")\n",
        "    for col in string_cols\n",
        "]\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_encoded\")\n",
        "    for col in string_cols\n",
        "]\n",
        "\n",
        "\n",
        "logistic_stages = indexers + encoders\n",
        "tree_stages = indexers\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "model = pipeline.fit(df)\n",
        "indexed_df = model.transform(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c49218a",
      "metadata": {
        "id": "6c49218a",
        "outputId": "c5e82acd-f397-49ad-e833-56a035994f9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train_df =spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\")\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cae30a8",
      "metadata": {
        "id": "2cae30a8",
        "outputId": "805b6239-529c-491c-ce93-8b3accfe5acd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+----+--------+------------+\n",
            "|canonical_smiles|drug|moa-fine|pca_features|\n",
            "+----------------+----+--------+------------+\n",
            "|               0|   0|       0|           0|\n",
            "+----------------+----+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum as _sum, when\n",
        "\n",
        "null_counts = train_df.select([\n",
        "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in train_df.columns\n",
        "])\n",
        "\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eacdabc",
      "metadata": {
        "id": "1eacdabc",
        "outputId": "e0b6c493-8582-466b-b7ff-8f44b0ff43a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 19:46:42 WARN DAGScheduler: Broadcasting large task binary with size 1210.5 KiB\n",
            "25/05/01 19:48:51 WARN DAGScheduler: Broadcasting large task binary with size 1275.5 KiB\n",
            "25/05/01 19:49:32 WARN DAGScheduler: Broadcasting large task binary with size 1309.8 KiB\n",
            "25/05/01 19:49:58 WARN DAGScheduler: Broadcasting large task binary with size 1331.7 KiB\n",
            "25/05/01 19:50:15 WARN DAGScheduler: Broadcasting large task binary with size 1345.7 KiB\n",
            "25/05/01 19:50:25 WARN DAGScheduler: Broadcasting large task binary with size 1346.4 KiB\n",
            "25/05/01 19:50:36 WARN DAGScheduler: Broadcasting large task binary with size 1329.6 KiB\n",
            "25/05/01 19:50:51 WARN DAGScheduler: Broadcasting large task binary with size 1387.4 KiB\n",
            "25/05/01 19:51:03 WARN DAGScheduler: Broadcasting large task binary with size 1345.0 KiB\n",
            "25/05/01 19:51:18 WARN DAGScheduler: Broadcasting large task binary with size 1351.8 KiB\n",
            "25/05/01 19:51:39 WARN DAGScheduler: Broadcasting large task binary with size 1359.7 KiB\n",
            "25/05/01 19:51:50 WARN DAGScheduler: Broadcasting large task binary with size 1350.7 KiB\n",
            "25/05/01 19:52:05 WARN DAGScheduler: Broadcasting large task binary with size 1363.8 KiB\n",
            "25/05/01 19:52:16 WARN DAGScheduler: Broadcasting large task binary with size 1343.1 KiB\n",
            "25/05/01 19:52:37 WARN DAGScheduler: Broadcasting large task binary with size 1324.8 KiB\n",
            "25/05/01 19:52:54 WARN DAGScheduler: Broadcasting large task binary with size 1336.9 KiB\n",
            "25/05/01 19:53:05 WARN DAGScheduler: Broadcasting large task binary with size 1332.4 KiB\n",
            "25/05/01 19:53:26 WARN DAGScheduler: Broadcasting large task binary with size 1345.6 KiB\n",
            "25/05/01 19:53:39 WARN DAGScheduler: Broadcasting large task binary with size 1321.1 KiB\n",
            "25/05/01 19:53:56 WARN DAGScheduler: Broadcasting large task binary with size 1328.6 KiB\n",
            "25/05/01 19:54:11 WARN DAGScheduler: Broadcasting large task binary with size 1358.7 KiB\n",
            "25/05/01 19:54:21 WARN DAGScheduler: Broadcasting large task binary with size 1349.2 KiB\n",
            "25/05/01 19:54:31 WARN DAGScheduler: Broadcasting large task binary with size 1211.4 KiB\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "\n",
        "categorical_cols = [\"canonical_smiles\",\"drug\"]\n",
        "indexed_cols = [col + \"_indexed\" for col in categorical_cols]\n",
        "\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col + \"_indexed\")\n",
        "    for col in categorical_cols\n",
        "]\n",
        "\n",
        "\n",
        "feature_cols = indexed_cols + [\"pca_features\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, label_indexer, rf])\n",
        "\n",
        "model = pipeline.fit(train_df) # use train df here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927cf927",
      "metadata": {
        "id": "927cf927"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test_df) # use test_df here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff1db25",
      "metadata": {
        "id": "4ff1db25",
        "outputId": "302a3599-0b53-47a0-cdea-eb6398edbdfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 19:55:43 WARN DAGScheduler: Broadcasting large task binary with size 1369.3 KiB\n",
            "25/05/01 19:55:55 WARN DAGScheduler: Broadcasting large task binary with size 1369.3 KiB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.5307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/01 19:56:05 WARN DAGScheduler: Broadcasting large task binary with size 1369.3 KiB\n",
            "25/05/01 19:56:14 WARN DAGScheduler: Broadcasting large task binary with size 1369.3 KiB\n",
            "[Stage 117:============================>                            (1 + 1) / 2]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score = 0.3764\n",
            "Precision = 0.3568\n",
            "Recall = 0.5307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7637ebd",
      "metadata": {
        "scrolled": true,
        "id": "c7637ebd",
        "outputId": "5d11f631-add4-47d5-9c5a-678bd68b6fbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 13:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|    canonical_smiles|                drug|            moa-fine|        pca_features|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|CNCC(C1=CC(=CC=C1...|Phenylephrine (hy...|Adrenoceptor agonist|[-111.78087590760...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train_df =spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\")\n",
        "train_df.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Leakage:\n",
        "### If we know a drug and its concentration, we cannot"
      ],
      "metadata": {
        "id": "NYfVpTBTn0t4"
      },
      "id": "NYfVpTBTn0t4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2e8a64",
      "metadata": {
        "id": "1b2e8a64",
        "outputId": "f986925b-64e0-41fa-cfd4-a2d9f2d8b6e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.9989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 171:>                                                      (0 + 20) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score = 0.9989\n",
            "Precision = 0.9989\n",
            "Recall = 0.9989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 171:====================================================>  (19 + 1) / 20]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100)\n",
        "    .getOrCreate())\n",
        "\n",
        "\n",
        "\n",
        "train_df =spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\")\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "\n",
        "train_df = train_df.filter(\n",
        "    (col(\"canonical_smiles\") != \"\") & (col(\"drug\") != \"\")\n",
        ")\n",
        "test_df = test_df.filter(\n",
        "    (col(\"canonical_smiles\") != \"\") & (col(\"drug\") != \"\")\n",
        ")\n",
        "\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
        "\n",
        "\n",
        "categorical_cols = [\"canonical_smiles\",\"drug\"]\n",
        "indexed_cols = [col + \"_indexed\" for col in categorical_cols]\n",
        "coded_cols = [col + \"_coded\" for col in categorical_cols]\n",
        "\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col + \"_indexed\")\n",
        "    for col in categorical_cols\n",
        "]\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_coded\")\n",
        "    for col in categorical_cols\n",
        "] # this causes problems because there are some none or empty values in the categorical cols\n",
        "\n",
        "feature_cols = indexed_cols + [\"pca_features\"]\n",
        "one_hot_features = coded_cols + [\"pca_features\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "one_hot_assembler = VectorAssembler(inputCols=one_hot_features, outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # L2 regularization\n",
        ")\n",
        "\n",
        "# mlp = MultilayerPerceptronClassifier(\n",
        "#     featuresCol=\"features\",\n",
        "#     labelCol=\"label_idx\",\n",
        "#     predictionCol=\"prediction\",\n",
        "#     maxIter=100,\n",
        "#     layers=[input_dim, 64, 32, 26],  # Define this based on your data\n",
        "#     blockSize=128,\n",
        "#     seed=42\n",
        "# )\n",
        "\n",
        "pipeline = Pipeline(stages=indexers +[assembler, label_indexer, lr])\n",
        "onehot_pipeline = Pipeline(stages= indexers + encoders + [one_hot_assembler, label_indexer, lr])\n",
        "\n",
        "#model = pipeline.fit(train_df)\n",
        "\n",
        "model = onehot_pipeline.fit(train_df)\n",
        "\n",
        "\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4a29f1",
      "metadata": {
        "id": "db4a29f1",
        "outputId": "fe3270cd-e33e-4aec-80d8-47f80c248797"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 54:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+--------+--------------------+------------------------+------------+\n",
            "|    canonical_smiles|       drug|moa-fine|        pca_features|canonical_smiles_indexed|drug_indexed|\n",
            "+--------------------+-----------+--------+--------------------+------------------------+------------+\n",
            "|C1=CC=C2C(=C1)N=C...|HI-TOPK-032| unclear|[-124.15845761817...|                   348.0|       350.0|\n",
            "+--------------------+-----------+--------+--------------------+------------------------+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "predictions.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a9263d",
      "metadata": {
        "id": "47a9263d",
        "outputId": "90d028b5-12d1-4b3a-96c4-547cd0ed97ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.select(\"canonical_smiles\").distinct().filter(\"canonical_smiles = ''\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1e9b2f",
      "metadata": {
        "id": "3b1e9b2f",
        "outputId": "d133184d-210e-4254-d541-22ae2b66048f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 57:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+--------+--------------------+------------------------+------------+----------------------+\n",
            "|    canonical_smiles|       drug|moa-fine|        pca_features|canonical_smiles_indexed|drug_indexed|canonical_smiles_coded|\n",
            "+--------------------+-----------+--------+--------------------+------------------------+------------+----------------------+\n",
            "|C1=CC=C2C(=C1)N=C...|HI-TOPK-032| unclear|[-124.15845761817...|                   348.0|       350.0|     (374,[348],[1.0])|\n",
            "+--------------------+-----------+--------+--------------------+------------------------+------------+----------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "onehot =OneHotEncoder(inputCol=\"canonical_smiles_indexed\", outputCol=\"canonical_smiles_coded\").fit(predictions)\n",
        "\n",
        "onehot.transform(predictions).show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc2c52f",
      "metadata": {
        "id": "4fc2c52f",
        "outputId": "1396e0d5-ffd6-4ebc-9be3-dbdd23ae5070"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 18:=============================>                            (1 + 1) / 2]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------+\n",
            "|canonical_smiles_indexed|\n",
            "+------------------------+\n",
            "|                   299.0|\n",
            "|                   323.0|\n",
            "|                   118.0|\n",
            "|                   305.0|\n",
            "|                   170.0|\n",
            "|                   147.0|\n",
            "|                   184.0|\n",
            "|                    71.0|\n",
            "|                   186.0|\n",
            "|                   160.0|\n",
            "|                   169.0|\n",
            "|                    67.0|\n",
            "|                   156.0|\n",
            "|                    70.0|\n",
            "|                   311.0|\n",
            "|                     8.0|\n",
            "|                   173.0|\n",
            "|                   143.0|\n",
            "|                   361.0|\n",
            "|                   320.0|\n",
            "+------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "predictions.select(\"canonical_smiles_indexed\").distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00687b18",
      "metadata": {
        "id": "00687b18",
        "outputId": "01660612-ee4d-4b56-c9db-63d0be54a710"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 15:=============================>                            (1 + 1) / 2]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+----+--------+------------+\n",
            "|canonical_smiles|drug|moa-fine|pca_features|\n",
            "+----------------+----+--------+------------+\n",
            "|               0|   0|       0|           0|\n",
            "+----------------+----+--------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum as _sum, when\n",
        "\n",
        "null_counts = predictions.select([\n",
        "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in train_df.columns\n",
        "])\n",
        "\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073c26bd",
      "metadata": {
        "id": "073c26bd"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4732af4",
      "metadata": {
        "id": "b4732af4",
        "outputId": "06bfbe95-4fb4-411c-93b5-33eba53438c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/02 14:39:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_23 !\n",
            "25/05/02 14:39:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_10 !\n",
            "25/05/02 14:39:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_27 !\n",
            "25/05/02 14:39:44 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_1 !\n",
            "25/05/02 14:39:45 WARN YarnAllocator: Container from a bad node: container_1746193490402_0001_01_000008 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:39:45.378]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:39:45.380]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:39:45.381]Killed by external signal\n",
            ".\n",
            "25/05/02 14:39:45 ERROR YarnScheduler: Lost executor 7 on cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal: Container from a bad node: container_1746193490402_0001_01_000008 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:39:45.378]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:39:45.380]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:39:45.381]Killed by external signal\n",
            ".\n",
            "25/05/02 14:39:45 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container from a bad node: container_1746193490402_0001_01_000008 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:39:45.378]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:39:45.380]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:39:45.381]Killed by external signal\n",
            ".\n",
            "25/05/02 14:39:45 WARN TaskSetManager: Lost task 1.0 in stage 96.0 (TID 1509) (cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746193490402_0001_01_000008 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:39:45.378]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:39:45.380]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:39:45.381]Killed by external signal\n",
            ".\n",
            "25/05/02 14:39:45 WARN TaskSetManager: Lost task 27.0 in stage 96.0 (TID 1517) (cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746193490402_0001_01_000008 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:39:45.378]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:39:45.380]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:39:45.381]Killed by external signal\n",
            ".\n",
            "25/05/02 14:44:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_25 !\n",
            "25/05/02 14:44:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_14 !\n",
            "25/05/02 14:44:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_13 !\n",
            "25/05/02 14:44:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_9 !\n",
            "25/05/02 14:44:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_22 !\n",
            "25/05/02 14:44:42 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_50_0 !\n",
            "25/05/02 14:44:43 WARN YarnAllocator: Container from a bad node: container_1746193490402_0001_01_000007 on host: cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:44:43.471]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:44:43.479]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:44:43.483]Killed by external signal\n",
            ".\n",
            "25/05/02 14:44:43 ERROR YarnScheduler: Lost executor 6 on cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal: Container from a bad node: container_1746193490402_0001_01_000007 on host: cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:44:43.471]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:44:43.479]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:44:43.483]Killed by external signal\n",
            ".\n",
            "25/05/02 14:44:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1746193490402_0001_01_000007 on host: cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:44:43.471]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:44:43.479]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:44:43.483]Killed by external signal\n",
            ".\n",
            "25/05/02 14:44:43 WARN TaskSetManager: Lost task 9.0 in stage 110.0 (TID 1755) (cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746193490402_0001_01_000007 on host: cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:44:43.471]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:44:43.479]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:44:43.483]Killed by external signal\n",
            ".\n",
            "25/05/02 14:44:43 WARN TaskSetManager: Lost task 28.0 in stage 110.0 (TID 1798) (cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746193490402_0001_01_000007 on host: cluster-bb8b-w-3.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:44:43.471]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:44:43.479]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:44:43.483]Killed by external signal\n",
            ".\n",
            "25/05/02 14:47:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_27 !\n",
            "25/05/02 14:47:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_10 !\n",
            "25/05/02 14:47:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_13 !\n",
            "25/05/02 14:47:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_9 !\n",
            "25/05/02 14:47:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_233_28 !\n",
            "25/05/02 14:47:18 WARN YarnAllocator: Container from a bad node: container_1746193490402_0001_01_000014 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:47:17.834]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:47:17.835]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:47:17.835]Killed by external signal\n",
            ".\n",
            "25/05/02 14:47:18 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 13 for reason Container from a bad node: container_1746193490402_0001_01_000014 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:47:17.834]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:47:17.835]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:47:17.835]Killed by external signal\n",
            ".\n",
            "25/05/02 14:47:18 ERROR YarnScheduler: Lost executor 13 on cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal: Container from a bad node: container_1746193490402_0001_01_000014 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:47:17.834]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:47:17.835]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:47:17.835]Killed by external signal\n",
            ".\n",
            "25/05/02 14:47:18 WARN TaskSetManager: Lost task 22.0 in stage 116.0 (TID 1897) (cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746193490402_0001_01_000014 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:47:17.834]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:47:17.835]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:47:17.835]Killed by external signal\n",
            ".\n",
            "25/05/02 14:47:18 WARN TaskSetManager: Lost task 29.0 in stage 116.0 (TID 1899) (cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1746193490402_0001_01_000014 on host: cluster-bb8b-w-4.us-central1-a.c.big-data-attempt-2-457220.internal. Exit status: 143. Diagnostics: [2025-05-02 14:47:17.834]Container killed on request. Exit code is 143\n",
            "[2025-05-02 14:47:17.835]Container exited with a non-zero exit code 143. \n",
            "[2025-05-02 14:47:17.835]Killed by external signal\n",
            ".\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.5341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 208:>                                                      (0 + 20) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score = 0.3777\n",
            "Precision = 0.3680\n",
            "Recall = 0.5341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100)\n",
        "    .getOrCreate())\n",
        "\n",
        "\n",
        "\n",
        "train_df =spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\")\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "\n",
        "train_df = train_df.filter(\n",
        "    (F.col(\"canonical_smiles\") != \"\") & (F.col(\"drug\") != \"\")\n",
        ")\n",
        "test_df = test_df.filter(\n",
        "    (F.col(\"canonical_smiles\") != \"\") & (F.col(\"drug\") != \"\")\n",
        ")\n",
        "\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
        "\n",
        "\n",
        "categorical_cols = [\"canonical_smiles\",\"drug\"]\n",
        "indexed_cols = [col + \"_indexed\" for col in categorical_cols]\n",
        "coded_cols = [col + \"_coded\" for col in categorical_cols]\n",
        "\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col + \"_indexed\")\n",
        "    for col in categorical_cols\n",
        "]\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_coded\")\n",
        "    for col in categorical_cols\n",
        "] # this causes problems because there are some none or empty values in the categorical cols\n",
        "\n",
        "feature_cols = [\"pca_features\"]\n",
        "one_hot_features = coded_cols + [\"pca_features\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "one_hot_assembler = VectorAssembler(inputCols=one_hot_features, outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # L2 regularization\n",
        ")\n",
        "\n",
        "# mlp = MultilayerPerceptronClassifier(\n",
        "#     featuresCol=\"features\",\n",
        "#     labelCol=\"label_idx\",\n",
        "#     predictionCol=\"prediction\",\n",
        "#     maxIter=100,\n",
        "#     layers=[input_dim, 64, 32, 26],  # Define this based on your data\n",
        "#     blockSize=128,\n",
        "#     seed=42\n",
        "# )\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, label_indexer, lr])\n",
        "onehot_pipeline = Pipeline(stages= indexers + encoders + [one_hot_assembler, label_indexer, lr])\n",
        "\n",
        "#model = pipeline.fit(train_df)\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881e3530",
      "metadata": {
        "id": "881e3530",
        "outputId": "8ce25d56-b1a0-4ca2-a6c5-7bd863b43154"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/02 21:38:34 WARN DAGScheduler: Broadcasting large task binary with size 1144.2 KiB\n",
            "25/05/02 21:41:02 WARN DAGScheduler: Broadcasting large task binary with size 1213.3 KiB\n",
            "25/05/02 21:41:56 WARN DAGScheduler: Broadcasting large task binary with size 1242.1 KiB\n",
            "25/05/02 21:42:29 WARN DAGScheduler: Broadcasting large task binary with size 1256.6 KiB\n",
            "25/05/02 21:42:49 WARN DAGScheduler: Broadcasting large task binary with size 1269.9 KiB\n",
            "25/05/02 21:43:02 WARN DAGScheduler: Broadcasting large task binary with size 1274.9 KiB\n",
            "25/05/02 21:43:16 WARN DAGScheduler: Broadcasting large task binary with size 1278.9 KiB\n",
            "25/05/02 21:43:37 WARN DAGScheduler: Broadcasting large task binary with size 1293.9 KiB\n",
            "25/05/02 21:43:51 WARN DAGScheduler: Broadcasting large task binary with size 1271.6 KiB\n",
            "25/05/02 21:44:04 WARN DAGScheduler: Broadcasting large task binary with size 1261.1 KiB\n",
            "25/05/02 21:44:38 WARN DAGScheduler: Broadcasting large task binary with size 1277.3 KiB\n",
            "25/05/02 21:44:58 WARN DAGScheduler: Broadcasting large task binary with size 1292.3 KiB\n",
            "25/05/02 21:45:12 WARN DAGScheduler: Broadcasting large task binary with size 1269.9 KiB\n",
            "25/05/02 21:45:26 WARN DAGScheduler: Broadcasting large task binary with size 1272.3 KiB\n",
            "25/05/02 21:45:47 WARN DAGScheduler: Broadcasting large task binary with size 1290.6 KiB\n",
            "25/05/02 21:46:01 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
            "25/05/02 21:46:14 WARN DAGScheduler: Broadcasting large task binary with size 1170.5 KiB\n",
            "25/05/02 21:46:32 WARN DAGScheduler: Broadcasting large task binary with size 1215.6 KiB\n",
            "25/05/02 21:46:54 WARN DAGScheduler: Broadcasting large task binary with size 1272.3 KiB\n",
            "25/05/02 21:47:16 WARN DAGScheduler: Broadcasting large task binary with size 1276.5 KiB\n",
            "25/05/02 21:47:30 WARN DAGScheduler: Broadcasting large task binary with size 1290.6 KiB\n",
            "25/05/02 21:47:45 WARN DAGScheduler: Broadcasting large task binary with size 1178.0 KiB\n",
            "25/05/02 21:47:52 WARN DAGScheduler: Broadcasting large task binary with size 1219.3 KiB\n",
            "[Stage 255:>                                                      (0 + 20) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of Workers: 5\n",
            "Model: Linear Regression\n",
            "Test Accuracy = 0.5211\n",
            "F1 Score = 0.3570\n",
            "Precision = 0.2715\n",
            "Recall = 0.5211\n",
            "Execution time: 760.87 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Example DataFrame operation\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "#Expression data 256 (63k to 256 from PCA) -> moa-fine\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100)\n",
        "    .getOrCreate())\n",
        "\n",
        "\n",
        "\n",
        "train_df = spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\") #10 m , 12.5% of dataset\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/twenty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/forty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/eighty_percent_subset/\")\n",
        "\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "feature_cols = [\"pca_features\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # L2 regularization\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, label_indexer, rf])\n",
        "\n",
        "\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(\"No. of Workers:\", 5)\n",
        "print(f\"Model:\", \"Linear Regression\")\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression & Random Forest 20% Data"
      ],
      "metadata": {
        "id": "N14LgmESPOyn"
      },
      "id": "N14LgmESPOyn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ff5c80",
      "metadata": {
        "id": "e7ff5c80",
        "outputId": "d8f9f1a6-484e-4005-d9fe-a1c597475051"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/02 22:10:34 WARN DAGScheduler: Broadcasting large task binary with size 1144.2 KiB\n",
            "25/05/02 22:14:08 WARN DAGScheduler: Broadcasting large task binary with size 1213.3 KiB\n",
            "25/05/02 22:15:35 WARN DAGScheduler: Broadcasting large task binary with size 1242.1 KiB\n",
            "25/05/02 22:16:26 WARN DAGScheduler: Broadcasting large task binary with size 1256.6 KiB\n",
            "25/05/02 22:16:58 WARN DAGScheduler: Broadcasting large task binary with size 1269.9 KiB\n",
            "25/05/02 22:17:20 WARN DAGScheduler: Broadcasting large task binary with size 1274.9 KiB\n",
            "25/05/02 22:17:42 WARN DAGScheduler: Broadcasting large task binary with size 1278.9 KiB\n",
            "25/05/02 22:18:15 WARN DAGScheduler: Broadcasting large task binary with size 1293.9 KiB\n",
            "25/05/02 22:18:38 WARN DAGScheduler: Broadcasting large task binary with size 1271.6 KiB\n",
            "25/05/02 22:19:00 WARN DAGScheduler: Broadcasting large task binary with size 1261.1 KiB\n",
            "25/05/02 22:19:52 WARN DAGScheduler: Broadcasting large task binary with size 1277.3 KiB\n",
            "25/05/02 22:20:25 WARN DAGScheduler: Broadcasting large task binary with size 1292.3 KiB\n",
            "25/05/02 22:20:47 WARN DAGScheduler: Broadcasting large task binary with size 1269.9 KiB\n",
            "25/05/02 22:21:09 WARN DAGScheduler: Broadcasting large task binary with size 1272.3 KiB\n",
            "25/05/02 22:21:43 WARN DAGScheduler: Broadcasting large task binary with size 1290.6 KiB\n",
            "25/05/02 22:22:05 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
            "25/05/02 22:22:28 WARN DAGScheduler: Broadcasting large task binary with size 1170.5 KiB\n",
            "25/05/02 22:22:57 WARN DAGScheduler: Broadcasting large task binary with size 1215.6 KiB\n",
            "25/05/02 22:23:33 WARN DAGScheduler: Broadcasting large task binary with size 1272.3 KiB\n",
            "25/05/02 22:24:08 WARN DAGScheduler: Broadcasting large task binary with size 1276.5 KiB\n",
            "25/05/02 22:24:31 WARN DAGScheduler: Broadcasting large task binary with size 1290.6 KiB\n",
            "25/05/02 22:24:53 WARN DAGScheduler: Broadcasting large task binary with size 1178.0 KiB\n",
            "25/05/02 22:25:05 WARN DAGScheduler: Broadcasting large task binary with size 1219.3 KiB\n",
            "[Stage 415:>                                                      (0 + 20) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20% Dataset\n",
            "No. of Workers: 5\n",
            "Model: Random Forest\n",
            "Test Accuracy = 0.5211\n",
            "F1 Score = 0.3570\n",
            "Precision = 0.2715\n",
            "Recall = 0.5211\n",
            "Execution time: 1103.79 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 415:====================================================>  (19 + 1) / 20]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Example DataFrame operation\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "#Expression data 256 (63k to 256 from PCA) -> moa-fine\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100)\n",
        "    .getOrCreate())\n",
        "\n",
        "\n",
        "\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\") #10 m , 12.5% of dataset\n",
        "train_df = spark.read.parquet(\"gs://bigdata_27/twenty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/forty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/eighty_percent_subset/\")\n",
        "\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "feature_cols = [\"pca_features\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # L2 regularization\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, label_indexer, rf])\n",
        "\n",
        "\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(\"20% Dataset\")\n",
        "print(\"No. of Workers:\", 5)\n",
        "print(f\"Model:\", \"Random Forest\")\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression & Random Forest 40% Data"
      ],
      "metadata": {
        "id": "LvxjRCiRPUHe"
      },
      "id": "LvxjRCiRPUHe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a4ce1d",
      "metadata": {
        "id": "30a4ce1d",
        "outputId": "4cfff5a9-9d70-45a9-ce01-a054db7a0389"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/02 22:37:26 WARN DAGScheduler: Broadcasting large task binary with size 1144.2 KiB\n",
            "25/05/02 22:50:50 WARN DAGScheduler: Broadcasting large task binary with size 1213.3 KiB\n",
            "25/05/02 22:56:56 WARN DAGScheduler: Broadcasting large task binary with size 1242.1 KiB\n",
            "25/05/02 23:01:30 WARN DAGScheduler: Broadcasting large task binary with size 1256.6 KiB\n",
            "25/05/02 23:04:31 WARN DAGScheduler: Broadcasting large task binary with size 1269.9 KiB\n",
            "25/05/02 23:07:15 WARN DAGScheduler: Broadcasting large task binary with size 1274.9 KiB\n",
            "25/05/02 23:10:08 WARN DAGScheduler: Broadcasting large task binary with size 1278.9 KiB\n",
            "25/05/02 23:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1293.9 KiB\n",
            "25/05/02 23:17:05 WARN DAGScheduler: Broadcasting large task binary with size 1271.6 KiB\n",
            "25/05/02 23:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1261.1 KiB\n",
            "25/05/02 23:24:26 WARN DAGScheduler: Broadcasting large task binary with size 1277.3 KiB\n",
            "25/05/02 23:27:40 WARN DAGScheduler: Broadcasting large task binary with size 1292.3 KiB\n",
            "25/05/02 23:30:16 WARN DAGScheduler: Broadcasting large task binary with size 1269.9 KiB\n",
            "25/05/02 23:33:16 WARN DAGScheduler: Broadcasting large task binary with size 1272.3 KiB\n",
            "25/05/02 23:36:40 WARN DAGScheduler: Broadcasting large task binary with size 1290.6 KiB\n",
            "25/05/02 23:39:36 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
            "25/05/02 23:42:22 WARN DAGScheduler: Broadcasting large task binary with size 1170.5 KiB\n",
            "25/05/02 23:49:28 WARN DAGScheduler: Broadcasting large task binary with size 1272.3 KiB\n",
            "25/05/02 23:53:24 WARN DAGScheduler: Broadcasting large task binary with size 1276.5 KiB\n",
            "25/05/02 23:56:26 WARN DAGScheduler: Broadcasting large task binary with size 1290.6 KiB\n",
            "25/05/02 23:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1178.0 KiB\n",
            "25/05/03 00:02:12 WARN DAGScheduler: Broadcasting large task binary with size 1219.3 KiB\n",
            "[Stage 567:>                                                      (0 + 20) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40% Dataset\n",
            "No. of Workers: 5\n",
            "Model: Logistic Regression\n",
            "Test Accuracy = 0.5245\n",
            "F1 Score = 0.3666\n",
            "Precision = 0.3508\n",
            "Recall = 0.5245\n",
            "Execution time: 7100.90 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 567:=====>                                                 (2 + 18) / 20]\r",
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Example DataFrame operation\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "#Expression data 256 (63k to 256 from PCA) -> moa-fine\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100)\n",
        "    .getOrCreate())\n",
        "\n",
        "\n",
        "\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\") #10 m , 12.5% of dataset\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/twenty_percent_subset/\")\n",
        "train_df = spark.read.parquet(\"gs://bigdata_27/forty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/eighty_percent_subset/\")\n",
        "\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "feature_cols = [\"pca_features\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # L2 regularization\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, label_indexer, rf])\n",
        "\n",
        "\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(\"40% Dataset\")\n",
        "print(\"No. of Workers:\", 5)\n",
        "print(f\"Model:\", \"Random Forest\")\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP 12.5% Dataset\n",
        "###Obervations: It was a simple single layer setup, took about 50 minutes, so we skipped running for scale up as we have to be mindful of our free credits and project minimum requirement"
      ],
      "metadata": {
        "id": "4wAgayDnPXxD"
      },
      "id": "4wAgayDnPXxD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9717da8f",
      "metadata": {
        "id": "9717da8f",
        "outputId": "a3e9ffbe-5619-48e7-dafc-118a258834f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/03 02:04:04 WARN BlockManager: Asked to remove block broadcast_1040_piece0, which does not exist\n",
            "[Stage 800:>                                                      (0 + 20) / 20]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Multilayer Perceptron Classifier\n",
            "Test Accuracy = 0.5223\n",
            "F1 Score = 0.3601\n",
            "Precision = 0.2874\n",
            "Recall = 0.5223\n",
            "Execution time: 2909.39 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Example DataFrame operation\n",
        "start_time = time.time()\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "train_df = spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\") #10 m , 12.5% of dataset\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/twenty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/forty_percent_subset/\")\n",
        "# train_df = spark.read.parquet(\"gs://bigdata_27/eighty_percent_subset/\")\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "\n",
        "# === Assemble Features ===\n",
        "def get_feature_pipeline(feature_cols, label_col=\"moa-fine\"):\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "    label_indexer = StringIndexer(inputCol=label_col, outputCol=\"label_index\")\n",
        "    return assembler, label_indexer\n",
        "\n",
        "# === Define MLPC Model ===\n",
        "def get_mlp_model(input_size, output_size):\n",
        "    layers = [input_size, 256, 128, output_size]  # Example architecture\n",
        "    mlp = MultilayerPerceptronClassifier(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label_index\",\n",
        "        predictionCol=\"prediction\",\n",
        "        maxIter=100,\n",
        "        layers=layers,\n",
        "        blockSize=128,\n",
        "        seed=47\n",
        "    )\n",
        "    return mlp\n",
        "\n",
        "# === Evaluation Metrics ===\n",
        "def evaluate_model(predictions):\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\")\n",
        "    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
        "    f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "    precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "    recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "    return accuracy, f1, precision, recall\n",
        "\n",
        "# === Main Pipeline Execution ===\n",
        "feature_cols = [\"pca_features\"]\n",
        "assembler, label_indexer = get_feature_pipeline(feature_cols)\n",
        "\n",
        "# Estimate number of classes\n",
        "num_classes = train_df.select(\"moa-fine\").distinct().count()\n",
        "print(num_classes)\n",
        "input_size = 256  # Since pca_features is assumed to be a single vector column\n",
        "\n",
        "mlp = get_mlp_model(input_size=input_size, output_size=num_classes)\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, label_indexer, mlp])\n",
        "model = pipeline.fit(train_df)\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "test_accuracy = evaluator.evaluate(predictions)\n",
        "accuracy, f1, precision, recall = evaluate_model(predictions)\n",
        "\n",
        "print(\"Model: Multilayer Perceptron Classifier\")\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")\n",
        "end_time = time.time()\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Above Setup to be run for Scale Out\n",
        "#We didn't do more than 12.5% of dataset with Random Forest as we were short on credits"
      ],
      "metadata": {
        "id": "zX6azc6QP-p0"
      },
      "id": "zX6azc6QP-p0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1f6af8",
      "metadata": {
        "id": "9e1f6af8",
        "outputId": "b6be1b46-23c1-4365-823d-756dc0c48665"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            moa-fine|count|\n",
            "+--------------------+-----+\n",
            "|             unclear| 4899|\n",
            "|DNA synthesis/rep...|  642|\n",
            "|Cyclooxygenase in...|  375|\n",
            "| EGFR/ERBB inhibitor|  372|\n",
            "|  Other TK inhibitor|  354|\n",
            "|  Multi-TK inhibitor|  244|\n",
            "|Adrenoceptor agonist|  219|\n",
            "|       RAS inhibitor|  208|\n",
            "|                    |  197|\n",
            "|Other MAPK inhibitor|  162|\n",
            "|      MTOR inhibitor|  155|\n",
            "|Microtubule inhib...|  149|\n",
            "|  JAK/STAT inhibitor|  138|\n",
            "|       MEK inhibitor|  137|\n",
            "|Protein synthesis...|  126|\n",
            "|  PI3K/AKT inhibitor|  125|\n",
            "|      HDAC inhibitor|  104|\n",
            "|Androgen receptor...|  102|\n",
            "|Glucocorticoid re...|  101|\n",
            "|       CDK inhibitor|   94|\n",
            "|Retinoic receptor...|   91|\n",
            "|Glucose transport...|   91|\n",
            "|       RAF inhibitor|   74|\n",
            "|DNA methyltransfe...|   66|\n",
            "|     Sonic inhibitor|   62|\n",
            "|Proteasome inhibitor|   59|\n",
            "|      GSK3 inhibitor|   56|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 811:============================>                            (1 + 1) / 2]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|       0.0| 9378|\n",
            "|      14.0|   20|\n",
            "|      12.0|    4|\n",
            "+----------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Check class distribution in test set\n",
        "test_df.groupBy(\"moa-fine\").count().orderBy(\"count\", ascending=False).show(30)\n",
        "\n",
        "# Check prediction distribution\n",
        "predictions.groupBy(\"prediction\").count().orderBy(\"count\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c02699",
      "metadata": {
        "id": "60c02699"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}