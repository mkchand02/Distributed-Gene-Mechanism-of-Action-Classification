{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "029533f4-543a-4da9-b86c-203bf51aed1d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rmsajALy3C-7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "from pyspark.ml.feature import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "dc8b39bb-dd3a-4c2c-b67a-43421605bd8e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "1Po3ucH13C-_"
      },
      "outputs": [],
      "source": [
        "# very important to keep number of partitions low, initially 3300 partitions of 70 Mbs each, reduced to 417 with below config, working with 3300 partitions resulted in 5x more compute time due to a lot of network I/O and led to workers being shut down.\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ControlPartitionSize\")\n",
        "    .config(\"spark.sql.files.maxPartitionBytes\", 734003200)\n",
        "    .config(\"spark.sql.shuffle.partitions\", 100)\n",
        "    .config(\"spark.hadoop.fs.gs.inputstream.buffer.size\", 1048576) # this reads data in 1 Mb chunks because there is a rate limit on reading data from GCP buckets on trial accounts.\n",
        "\n",
        "    .getOrCreate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7b03e77f-ccac-48f3-a78c-5b3db0df9c03",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "aUGKfYCU3C_A"
      },
      "outputs": [],
      "source": [
        "expression_df = spark.read.parquet(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b989bcbc-d03f-4f3e-b12f-40b9b15fe13a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "93BjNqOG3C_B"
      },
      "outputs": [],
      "source": [
        "# Major bottleneck in terms of performance, no alternative available,\n",
        "# potentially F.zipwith could be used but did not work in this case.\n",
        "def make_sparse_vector(genes, expressions):\n",
        "    if genes is None or expressions is None:\n",
        "        return SparseVector(63000, {})\n",
        "    return SparseVector(63000, dict(zip(genes, expressions)))\n",
        "make_sparse_vector_udf = udf(make_sparse_vector, VectorUDT())\n",
        "df.withColumn(\"features\", make_sparse_vector_udf(\"genes\", \"expressions\")).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://bigdata_27/features/\") #After every major computation,\n",
        "    # data was persisted on GCP bucket to avoid recomputing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ed8d0b8e-6493-441a-9fb5-fee8141c38ea",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8XdB-nek3C_B"
      },
      "outputs": [],
      "source": [
        "df = spark.read.parquet(\"gs://bigdata_27/features/\")\n",
        "\n",
        "# Variance threshold Selector was used to reduce the column size from\n",
        "# 63000 (63k) to 6250, given the scale of the task it was quite fast,\n",
        "# took 4-5 hrs to complete. Variance threshold Selector was used\n",
        "# because PCA could not be run on 63000 colums as it required atleast 15GBs driver memory\n",
        "selector = VarianceThresholdSelector(\n",
        "    varianceThreshold=0.10,\n",
        "    featuresCol=\"features\",\n",
        "    outputCol=\"selectedFeatures\"\n",
        ")\n",
        "\n",
        "selector_model = selector.fit(df)\n",
        "selector_model.transform(df).drop(\"features\").write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://bigdata_27/selected_features/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ad5ca9b5-9698-4f4f-8bea-9d9cfce3d585",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "oE9OU1TP3C_D"
      },
      "outputs": [],
      "source": [
        "# The data was sampled such that the target variable distribution remains\n",
        "# unchanged so that features are representative for testing as well as for\n",
        "# training a PCA model on a smaller subset. lit(0.01) selects 1% of the data,\n",
        "# this value was changed from 0.01%, 1%, 12.5%, 25%, 50% all the subsets were\n",
        "# written to GCP to avoid redoing this computation.\n",
        "\n",
        "fractions = scaled_df.select(\"moa-fine\").distinct().withColumn(\"fraction\", lit(0.01)).rdd.collectAsMap()\n",
        "sampled_df = scaled_df.sampleBy(\"moa-fine\", fractions, seed=42).coalesce(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7ba1588d-5dba-4487-84a5-3901fcaa4536",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "KibGSTbH3C_D"
      },
      "outputs": [],
      "source": [
        "# PCA was trained on 1% data 950k rows, large amount of partitions caused\n",
        "# problems here as well so they reduced as well. This process took 5 hours of computation\n",
        "sampled_df = spark.read.parquet(\"gs://bigdata_27/sampled_data/\")\n",
        "pca = PCA(k=256, inputCol=\"selected_features\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(sampled_df)\n",
        "\n",
        "# Save the PCA model\n",
        "pca_model.write().overwrite().save(\"gs://bigdata_27/pca_models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "76aed1e4-3bd6-4c99-b73d-cb35a15a4be6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6PA5HGGj3C_D"
      },
      "outputs": [],
      "source": [
        "pca_model = PCAModel.load(\"gs://bigdata_27/pca_models/\")\n",
        "sampled_df = pca_model.transform(sampled_df)\n",
        "sampled_df.select(\"pca_features\").show(1)\n",
        "pca_model.transform(scaled_df).drop(\"selected_features\").write.mode(\"overwrite\").parquet(\"gs://bigdata_27/transformed_data/\") # Data was written whenever a major computation was taking place\n",
        "transformed_df = spark.read.parquet(\"gs://bigdata_27/transformed_data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7d8181f5-6b65-427a-a89a-f59c5c774583",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8Y0coZah3C_E"
      },
      "outputs": [],
      "source": [
        "# data is being read from specific directories in buckets depending on the\n",
        "# experiment. Test row has 10k rows 0.01% of data, since 10k is larger enough\n",
        "# sample size for good statistical confidance.\n",
        "train_df =spark.read.parquet(\"gs://bigdata_27/ten_percent_subset/\")\n",
        "test_df = spark.read.parquet(\"gs://bigdata_27/test_data/\")\n",
        "from pyspark.sql.functions import col, sum as _sum, when\n",
        "\n",
        "null_counts = train_df.select([\n",
        "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in train_df.columns\n",
        "])\n",
        "\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spark Model Development Pipeline"
      ],
      "metadata": {
        "id": "NELG7o7Xii4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "771f1a33-d450-454e-b1fd-2e2940e5eadf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "w0Ho0gwM3C_F"
      },
      "outputs": [],
      "source": [
        "# Initially one hot encoder and string indexer were used for some categoricall columns, but they were resulting in data leakage since the model was scoring 99.8 F1 score on 12.5 % of data, therefore they were removed and the new pipeline is a much simpler version of what was used before.\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
        "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier, LogisticRegression\n",
        "\n",
        "\n",
        "feature_cols = [\"pca_features\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"moa-fine\", outputCol=\"label_index\")\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxBins=512,\n",
        "    numTrees=100\n",
        ")\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0  # L2 regularization\n",
        ")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, label_indexer, lr])\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "\n",
        "predictions = model.transform(test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Evaluation"
      ],
      "metadata": {
        "id": "Kq89xRIlifE8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "96b6aba8-6a1a-4778-92e1-8b4d694cf48f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "lhotPbSe3C_G"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "print(f\"F1 Score = {f1:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall = {recall:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "1"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "TeamD_BigData",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}