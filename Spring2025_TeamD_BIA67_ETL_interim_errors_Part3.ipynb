{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00174977",
      "metadata": {
        "id": "00174977"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf,lit,col\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PCA-Cluster-Job\")\\\n",
        "    .getOrCreate()\n",
        "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 800 * 1024 * 1024) #Partition size shouldn't exceed 800 mb\n",
        "\"\"\"Description of above configuration:\n",
        "\n",
        "\"\"\"\n",
        "spark.conf.set(\"spark.hadoop.fs.gs.inputstream.buffer.size\", 1048576) # 1MB data extracted at a time from GCP API to overcome rate limit issue\n",
        "\"\"\"Description of above configuration:\n",
        "sets a Spark configuration parameter that controls the buffer size (in bytes) used when reading data from Google Cloud Storage (GCS) using the gs:// URI scheme.\n",
        "Breakdown:\n",
        "\n",
        "    spark.conf.set(...): Sets a Spark runtime configuration.\n",
        "    fs.gs.inputstream.buffer.size: A Hadoop-GCS connector setting that specifies how much data (in bytes) is buffered when reading from GCS.\n",
        "    1048576: This is 1 MB (1024 * 1024 bytes).\n",
        "\n",
        "Effect:\n",
        "This increases the read buffer size to 1 MB, which can:\n",
        "    Improve performance by reducing the number of HTTP requests to GCS.\n",
        "    Be helpful when reading large files or working with high-latency networks.\n",
        "    The default is usually 8192 bytes (8 KB), which is small for large-scale distributed reads.\n",
        "Reason:\n",
        "    This is done to improve overall throughput.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9323fc5",
      "metadata": {
        "id": "c9323fc5",
        "outputId": "83213326-fa2b-42dd-a97b-85327a06b74a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "expression_df = spark.read.parquet(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/data/\")\n",
        "cell_line_df = spark.read.parquet(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/meta_data/cell_line_metadata.parquet\")\n",
        "drug_df = spark.read.parquet(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/meta_data/drug_metadata.parquet\")\n",
        "sample_df = spark.read.parquet(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/meta_data/sample_metadata.parquet\")\n",
        "df = expression_df.alias(\"expressions\").join(cell_line_df.alias(\"cell_line\"), expression_df.cell_line_id == cell_line_df.Cell_ID_Cellosaur)\\\n",
        "    .join(sample_df.alias(\"sample\"), \"sample\")\\\n",
        "    .join(drug_df.alias(\"drug\"), \"drug\")\\\n",
        "    .select('genes', 'expressions','expressions.moa-fine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95ed42a",
      "metadata": {
        "id": "f95ed42a",
        "outputId": "251a3303-380d-4929-8b88-8a4bdb37fb2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "fractions = df.select(col(\"moa-fine\")).distinct().withColumn(\"fraction\", lit(0.1)).rdd.collectAsMap()\n",
        "df.sampleBy(col(\"moa-fine\"), fractions, seed=42).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://medical-data-for-project/SUBSET/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d17c95",
      "metadata": {
        "id": "57d17c95",
        "outputId": "a0a6fd8e-50ac-4865-d24f-ab8161dfd4f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "90877981"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stratified = spark.read.parquet(\"gs://medical-data-for-project/SUBSET/\")\n",
        "stratified.select(col(\"moa-fine\")).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b31790de",
      "metadata": {
        "id": "b31790de",
        "outputId": "fc25f3f7-528e-4c0e-f41f-2fc9ad631bdb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "[Stage 21:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------------+\n",
            "|               genes|         expressions|          moa-fine|\n",
            "+--------------------+--------------------+------------------+\n",
            "|[1, 5, 11, 19, 25...|[-2.0, 1.0, 4.0, ...|Other TK inhibitor|\n",
            "+--------------------+--------------------+------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "stratified.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8d7b56",
      "metadata": {
        "id": "ac8d7b56"
      },
      "outputs": [],
      "source": [
        "def make_sparse_vector(genes, expressions):\n",
        "    if genes is None or expressions is None:\n",
        "        return SparseVector(63000, {})\n",
        "    return SparseVector(63000, dict(zip(genes, expressions)))\n",
        "make_sparse_vector_udf = udf(make_sparse_vector, VectorUDT())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e517a6a",
      "metadata": {
        "id": "9e517a6a"
      },
      "outputs": [],
      "source": [
        "stratified_features = stratified.withColumn(\"features\", make_sparse_vector_udf(\"genes\", \"expressions\")).select(\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe82fb8b",
      "metadata": {
        "id": "fe82fb8b",
        "outputId": "92006a3e-dad5-4374-b5fe-afcbaf1fbbac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/22 21:16:22 WARN RowMatrix: 63000 columns will require at least 31752 megabytes of memory!\n",
            "[Stage 29:>                                                      (0 + 20) / 100]\r"
          ]
        }
      ],
      "source": [
        "pca = PCA(k=256, inputCol=\"features\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(stratified_features)\n",
        "pca_model.save(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/pca_models/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318a9b94",
      "metadata": {
        "id": "318a9b94",
        "outputId": "3a39f9f2-9372-4a7a-a288-ac88cac477e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:14:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
            "25/04/23 02:15:29 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/04/23 02:15:34 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://medical-data-for-project/features_subset.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n",
            "\t... 17 more\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:15:40 WARN TaskSetManager: Lost task 7.0 in stage 1.0 (TID 8) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/04/23 02:15:56 WARN TaskSetManager: Lost task 20.0 in stage 1.0 (TID 21) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/04/23 02:16:07 WARN TaskSetManager: Lost task 10.1 in stage 1.0 (TID 31) (cluster-c6a6-w-3.us-central1-a.c.excellent-math-456021-s0.internal executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:10 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 25) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 3): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://medical-data-for-project/features_subset.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n",
            "\t... 17 more\n",
            "\n",
            "25/04/23 02:16:17 WARN TaskSetManager: Lost task 13.1 in stage 1.0 (TID 40) (cluster-c6a6-w-3.us-central1-a.c.excellent-math-456021-s0.internal executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/04/23 02:16:28 WARN TaskSetManager: Lost task 1.2 in stage 1.0 (TID 44) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:39 WARN TaskSetManager: Lost task 8.2 in stage 1.0 (TID 56) (cluster-c6a6-w-4.us-central1-a.c.excellent-math-456021-s0.internal executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "25/04/23 02:16:42 WARN TaskSetManager: Lost task 0.2 in stage 1.0 (TID 54) (cluster-c6a6-w-0.us-central1-a.c.excellent-math-456021-s0.internal executor 5): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to gs://medical-data-for-project/features_subset.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:493)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:476)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:483)\n",
            "\t... 17 more\n",
            "\n",
            "25/04/23 02:16:45 ERROR TaskSetManager: Task 15 in stage 1.0 failed 4 times; aborting job\n",
            "25/04/23 02:16:45 ERROR FileFormatWriter: Aborting job 79b27856-e005-4d59-8263-ef3654472b32.\n",
            "org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2452) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:380) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:329) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:377) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:197) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:473) ~[spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76) [spark-sql-api_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:473) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:449) [spark-catalyst_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802) [spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) [py4j-0.10.9.7.jar:?]\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374) [py4j-0.10.9.7.jar:?]\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282) [py4j-0.10.9.7.jar:?]\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) [py4j-0.10.9.7.jar:?]\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79) [py4j-0.10.9.7.jar:?]\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) [py4j-0.10.9.7.jar:?]\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) [py4j-0.10.9.7.jar:?]\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.18.jar:?]\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) ~[?:?]\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100) ~[spark-sql_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.3.jar:3.5.3]\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n",
            "\t... 1 more\n",
            "25/04/23 02:16:45 WARN TaskSetManager: Lost task 16.3 in stage 1.0 (TID 81) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:45 WARN TaskSetManager: Lost task 7.3 in stage 1.0 (TID 74) (cluster-c6a6-w-4.us-central1-a.c.excellent-math-456021-s0.internal executor 8): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:45 WARN TaskSetManager: Lost task 14.2 in stage 1.0 (TID 62) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:46 WARN TaskSetManager: Lost task 0.3 in stage 1.0 (TID 80) (cluster-c6a6-w-3.us-central1-a.c.excellent-math-456021-s0.internal executor 9): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "[Stage 1:>                                                       (0 + 16) / 445]\r"
          ]
        },
        {
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n    for batch in iterator:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n    batch = self._create_batch(series)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries([\n\u001b[1;32m     21\u001b[0m         SparseVector(\u001b[38;5;241m63000\u001b[39m, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(genes, expressions)))\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m genes, expressions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(genes_series, expressions_series)\n\u001b[1;32m     23\u001b[0m     ])\n\u001b[1;32m     25\u001b[0m stratified_features \u001b[38;5;241m=\u001b[39m stratified\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, make_sparse_vector_udf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpressions\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[43mstratified_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://medical-data-for-project/features_subset/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n    for batch in iterator:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n    batch = self._create_batch(series)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 17.3 in stage 1.0 (TID 78) (cluster-c6a6-w-0.us-central1-a.c.excellent-math-456021-s0.internal executor 5): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 13.2 in stage 1.0 (TID 61) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 2.3 in stage 1.0 (TID 65) (cluster-c6a6-w-3.us-central1-a.c.excellent-math-456021-s0.internal executor 9): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 11.3 in stage 1.0 (TID 79) (cluster-c6a6-w-0.us-central1-a.c.excellent-math-456021-s0.internal executor 5): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 12.3 in stage 1.0 (TID 72) (cluster-c6a6-w-4.us-central1-a.c.excellent-math-456021-s0.internal executor 6): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 20.2 in stage 1.0 (TID 63) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 4): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:47 WARN TaskSetManager: Lost task 19.3 in stage 1.0 (TID 73) (cluster-c6a6-w-3.us-central1-a.c.excellent-math-456021-s0.internal executor 10): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 3.3 in stage 1.0 (TID 71) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 5.3 in stage 1.0 (TID 66) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 3): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 4.3 in stage 1.0 (TID 68) (cluster-c6a6-w-3.us-central1-a.c.excellent-math-456021-s0.internal executor 10): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 10.3 in stage 1.0 (TID 70) (cluster-c6a6-w-1.us-central1-a.c.excellent-math-456021-s0.internal executor 4): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 1.3 in stage 1.0 (TID 67) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 8.3 in stage 1.0 (TID 76) (cluster-c6a6-w-0.us-central1-a.c.excellent-math-456021-s0.internal executor 7): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "\r\n",
            "[Stage 1:>                                                        (0 + 3) / 445]\r\n",
            "25/04/23 02:16:48 WARN TaskSetManager: Lost task 6.3 in stage 1.0 (TID 69) (cluster-c6a6-w-4.us-central1-a.c.excellent-math-456021-s0.internal executor 8): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 02:16:49 WARN TaskSetManager: Lost task 18.3 in stage 1.0 (TID 75) (cluster-c6a6-w-4.us-central1-a.c.excellent-math-456021-s0.internal executor 6): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n",
            "25/04/23 02:16:49 WARN TaskSetManager: Lost task 9.3 in stage 1.0 (TID 77) (cluster-c6a6-w-0.us-central1-a.c.excellent-math-456021-s0.internal executor 7): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 15 in stage 1.0 failed 4 times, most recent failure: Lost task 15.3 in stage 1.0 (TID 64) (cluster-c6a6-w-2.us-central1-a.c.excellent-math-456021-s0.internal executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
            "    process()\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
            "    serializer.dump_stream(out_iter, outfile)\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
            "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
            "    for batch in iterator:\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 464, in init_stream_yield_batches\n",
            "    batch = self._create_batch(series)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 444, in _create_batch\n",
            "    raise PySparkValueError(\n",
            "pyspark.errors.exceptions.base.PySparkValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
            "\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
            "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:458)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf,lit,col\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PCA-Cluster-Job\")\\\n",
        "    .getOrCreate()\n",
        "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 800 * 1024 * 1024)\n",
        "spark.conf.set(\"spark.hadoop.fs.gs.inputstream.buffer.size\", 1048576)\n",
        "\n",
        "stratified = spark.read.parquet(\"gs://medical-data-for-project/SUBSET/\")\n",
        "\n",
        "@pandas_udf(VectorUDT())\n",
        "def make_sparse_vector_udf(genes_series: pd.Series, expressions_series: pd.Series) -> pd.Series:\n",
        "    return pd.Series([\n",
        "        SparseVector(63000, dict(zip(genes, expressions)))\n",
        "        for genes, expressions in zip(genes_series, expressions_series)\n",
        "    ])\n",
        "\n",
        "stratified_features = stratified.withColumn(\"features\", make_sparse_vector_udf(\"genes\", \"expressions\")).select(\"features\")\n",
        "\n",
        "stratified_features.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"gs://medical-data-for-project/features_subset/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bef0353",
      "metadata": {
        "id": "2bef0353",
        "outputId": "25828302-6f68-40a9-c60a-6440426b8446"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'TruncatedSVD' from 'pyspark.ml.feature' (/usr/lib/spark/python/pyspark/ml/feature.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TruncatedSVD' from 'pyspark.ml.feature' (/usr/lib/spark/python/pyspark/ml/feature.py)"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddf3f30",
      "metadata": {
        "id": "5ddf3f30",
        "outputId": "ae5ae43c-b098-491d-e7ff-f55e557890b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(63000,[1,11,20,5...|\n",
            "+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "features = spark.read.parquet(\"gs://medical-data-for-project/features_subset/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PCA Another Failed Attempt with Row Matrix : Driver and Worker memory issues"
      ],
      "metadata": {
        "id": "FhV-mDqU1psI"
      },
      "id": "FhV-mDqU1psI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3efce5",
      "metadata": {
        "id": "5a3efce5",
        "outputId": "6a10c76a-d862-4399-e870-81d09fd9e845"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 13:58:19 WARN RowMatrix: 63000 columns will require at least 31752 megabytes of memory!\n",
            "25/04/23 14:25:48 WARN RowMatrix: 63000 columns will require at least 31752 megabytes of memory!\n"
          ]
        },
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Cannot aggregate object of size 15876252000 Bytes, as it's bigger than maxResultSize (1073741824 Bytes)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Cannot aggregate object of size 15876252000 Bytes, as it's bigger than maxResultSize (1073741824 Bytes)"
          ]
        }
      ],
      "source": [
        "pca = PCA(k=256, inputCol=\"features\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aea5ac4",
      "metadata": {
        "id": "2aea5ac4"
      },
      "outputs": [],
      "source": [
        "pca_model.save(\"gs://medical-data-for-project/huggingface.co/datasets/vevotx/Tahoe-100M/resolve/main/pca_models/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6dea71",
      "metadata": {
        "id": "dd6dea71",
        "outputId": "6ef9afc1-70c1-4b77-b546-e9ce1476f9be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/23 14:46:53 WARN RowMatrix: 63000 columns will require at least 31752 megabytes of memory!\n",
            "ERROR:root:KeyboardInterrupt while sending command.             (19 + 20) / 445]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Now this will work\u001b[39;00m\n\u001b[1;32m      9\u001b[0m mat \u001b[38;5;241m=\u001b[39m RowMatrix(vector_rdd)\n\u001b[0;32m---> 10\u001b[0m pc_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputePrincipalComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/mllib/linalg/distributed.py:466\u001b[0m, in \u001b[0;36mRowMatrix.computePrincipalComponents\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcomputePrincipalComponents\u001b[39m(\u001b[38;5;28mself\u001b[39m, k: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Matrix:\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    Computes the k principal components of the given row matrix\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    DenseVector([-4.6102, -4.9745])]\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_matrix_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomputePrincipalComponents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/mllib/common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39ma: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/spark/python/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
            "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.linalg.distributed import RowMatrix\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.mllib.linalg import Vectors as MLLibVectors\n",
        "\n",
        "# Convert each ml.linalg.SparseVector to mllib vector\n",
        "vector_rdd = features.select(\"features\").rdd.map(lambda row: MLLibVectors.fromML(row[0]))\n",
        "\n",
        "# Now this will work\n",
        "mat = RowMatrix(vector_rdd)\n",
        "pc_matrix = mat.computePrincipalComponents(k=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1001243",
      "metadata": {
        "id": "e1001243"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}